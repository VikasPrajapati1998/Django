{% load static %}
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Artificial Intelligence & Deep Learning</title>
    <link rel="stylesheet" href="{% static 'css/styles.css' %}">
</head>
<body>
    <header>
        <h1>Artificial Intelligence & Deep Learning</h1>
        <nav>
            <ul>
                <li><a href="#history">History</a></li>
                <li><a href="#present">Present</a></li>
                <li><a href="#future">Future</a></li>
                <li><a href="#types">Neural Networks</a></li>
            </ul>
        </nav>
    </header>

    <section id="history">
        <h2>History of AI & ML</h2>
        <p>
            Artificial Intelligence has been around since the mid-20th century. In 1956, the term "Artificial Intelligence" was coined, and since then, the field has seen massive growth... </br>
            The history of artificial intelligence (AI) began in antiquity, with myths, stories and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. 
            The study of logic and formal reasoning from antiquity to the present led directly to the invention of the programmable digital computer in the 1940s, a machine based on the abstract 
            essence of mathematical reasoning. This device and the ideas behind it inspired a handful of scientists to begin seriously discussing the possibility of building an electronic brain.
            The field of AI research was founded at a workshop held on the campus of Dartmouth College during the summer of 1956. Attendees of the workshop became the leaders of AI research for
             decades. Many of them predicted that machines as intelligent as humans would exist within a generation. The U.S. government provided millions of dollars to make this vision come true.
            Eventually, it became obvious that researchers had grossly underestimated the difficulty of the project. In 1974, criticism from James Lighthill and pressure from the U.S. Congress 
            led the U.S. and British Governments to stop funding undirected research into artificial intelligence. Seven years later, a visionary initiative by the Japanese Government and the success 
            of expert systems reinvigorated investment in AI and by the late 80s the industry had grown into the billions of dollars. However, investors' enthusiasm waned in the 1990s and the field was 
            criticized in the press and avoided by industry (a period known as the "AI Winter"). Nevertheless, research and funding continued to grow under other names.</br>
            In the early 2000s, machine learning was applied to a wide range of problems in academic and industry. The success was due to the availability of powerful computer hardware, the collection 
            of immense data sets and the application of solid mathematical methods. In 2012, deep learning proved to be a breakthrough technology, eclipsing all other methods. The transformer architecture 
            debuted in 2017 and was used to produce impressive generative AI applications. Investment in AI boomed in the 2020s.
        </p>
        <img src="{% static 'images/history.jpg' %}" alt="History of AI">
    </section>

    <section id="present">
        <h2>The Present of AI & ML</h2>
        <p>
            Today, AI and Machine Learning are used in numerous domains such as healthcare, finance, autonomous driving, and more. With advancements in neural networks and deep learning...<br>
            Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops 
            and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined 
            goals. Such machines may be called AIs.<br>
            Some high-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); interacting via human 
            speech (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT, and AI art); and superhuman play and analysis in 
            strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: "A lot of cutting edge AI has filtered into general applications, often without being called 
            AI because once something becomes useful enough and common enough it's not labeled AI anymore."<br>
            The various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, 
            planning, learning, natural language processing, perception, and support for robotics.<br>
            [a] General intelligence—the ability to complete any task performable by a human on an at least equal 
            level—is among the field's long-term goals.[4] To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, 
            formal logic, artificial neural networks, and methods based on statistics, operations research, and economics.<br>
            [b] AI also draws upon psychology, linguistics, philosophy, neuroscience, and 
            other fields.<br>
            Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism, followed by periods of disappointment and loss of funding, 
            known as AI winter. Funding and interest vastly increased after 2012 when deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer 
            architecture,[12] and by the early 2020s hundreds of billions of dollars were being invested in AI (known as the "AI boom"). The widespread use of AI in the 21st century exposed several 
            unintended consequences and harms in the present and raised concerns about its risks and long-term effects in the future, prompting discussions about regulatory policies to ensure the safety 
            and benefits of the technology.
        </p>
        <img src="{% static 'images/present.jpg' %}" alt="Present of AI">
    </section>

    <section id="future">
        <h2>The Future of AI & ML</h2>
        <p>
            The future of AI and ML is exciting with concepts like Artificial General Intelligence (AGI) and more specialized neural networks. It promises a world of automation, innovation...<br>
            The future is the time after the past and present. Its arrival is considered inevitable due to the existence of time and the laws of physics. Due to the apparent nature of reality and the 
            unavoidability of the future, everything that currently exists and will exist can be categorized as either permanent, meaning that it will exist forever, or temporary, meaning that it will 
            end. In the Occidental view, which uses a linear conception of time, the future is the portion of the projected timeline that is anticipated to occur. In special relativity, the future 
            is considered absolute future, or the future light cone.<br>
            In the philosophy of time, presentism is the belief that only the present exists and the future and the past are unreal. Religions consider the future when they address issues such as karma, 
            life after death, and eschatologies that study what the end of time and the end of the world will be. Religious figures such as prophets and diviners have claimed to see into the future. Future 
            studies, or futurology, is the science, art, and practice of postulating possible futures. Modern practitioners stress the importance of alternative and plural futures, rather than one monolithic 
            future, and the limitations of prediction and probability, versus the creation of possible and preferable futures. Predeterminism is the belief that the past, present, and future have been already 
            decided.<br>
            The concept of the future has been explored extensively in cultural production, including art movements and genres devoted entirely to its elucidation, such as the 20th-century movement futurism.
        </p>
        <img src="{% static 'images/future.jpg' %}" alt="Future of AI">
    </section>

    <section id="types">
        <h2>Types of Neural Networks</h2>
        <div class="network">
            <h3>Convolutional Neural Networks (CNN)</h3>
            <p>
                CNNs are primarily used in image processing tasks such as object detection, face recognition, etc.<br>
                Convolutional neural networks use three-dimensional data for image classification and object recognition tasks.
                Neural networks are a subset of machine learning, and they are at the heart of deep learning algorithms. They are comprised of node layers, containing an input layer, one or more hidden layers, and an 
                output layer. Each node connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data 
                to the next layer of the network. Otherwise, no data is passed along to the next layer of the network. <br>
                While we primarily focused on feedforward networks in that article, there are various types of neural nets, which are used for different use cases and data types. For example, recurrent neural networks 
                are commonly used for natural language processing and speech recognition whereas convolutional neural networks (ConvNets or CNNs) are more often utilized for classification and computer vision tasks. Prior 
                to CNNs, manual, time-consuming feature extraction methods were used to identify objects in images. However, convolutional neural networks now provide a more scalable approach to image classification and 
                object recognition tasks, leveraging principles from linear algebra, specifically matrix multiplication, to identify patterns within an image. That said, they can be computationally demanding, requiring 
                graphical processing units (GPUs) to train models. <br>
            </p>
            <img src="{% static 'images/cnn.jpg' %}" alt="CNN Neural Network">
        </div>
        <div class="network">
            <h3>Recurrent Neural Networks (RNN)</h3>
            <p>
                RNNs are used for sequential data like time series, natural language processing, and more.<br>
                A recurrent neural network or RNN is a deep neural network trained on sequential or time series data to create a machine learning (ML) model that can make sequential predictions or conclusions based on sequential inputs.<br>
                An RNN might be used to predict daily flood levels based on past daily flood, tide and meteorological data. But RNNs can also be used to solve ordinal or temporal problems such as language translation, natural 
                language processing (NLP), sentiment analysis, speech recognition and image captioning.<br>
            </p>
            <img src="{% static 'images/rnn.jpg' %}" alt="RNN Neural Network">
        </div>
        <div class="network">
            <h3>Generative Adversarial Networks (GAN)</h3>
            <p>
                GANs are a class of AI models used to generate new data, like creating realistic images, music, and more.<br>
                Generative AI relies on sophisticated machine learning models called deep learning models—algorithms that simulate the learning and decision-making processes of the human brain. These models work by identifying and encoding the 
                patterns and relationships in huge amounts of data, and then using that information to understand users' natural language requests or questions and respond with relevant new content. <br>
                AI has been a hot technology topic for the past decade, but generative AI, and specifically the arrival of ChatGPT in 2022, has thrust AI into worldwide headlines and launched an unprecedented surge of AI innovation and adoption. 
                Generative AI offers enormous productivity benefits for individuals and organizations, and while it also presents very real challenges and risks, businesses are forging ahead, exploring how the technology can improve their internal 
                workflows and enrich their products and services. According to research by the management consulting firm McKinsey, one third of organizations are already using generative AI regularly in at least one business function. Industry 
                analyst Gartner projects more than 80% of organizations will have deployed generative AI applications or used generative AI application programming interfaces (APIs) by 2026.<br>
            </p>
            <img src="{% static 'images/gan.jpg' %}" alt="GAN Neural Network">
        </div>
    </section>

    <section id="extra">
        <h2>Did You Know?</h2>
        <p id="ai-fact"></p>
    </section>

    <footer>
        <p>&copy; 2024 AI & Deep Learning. All Rights Reserved.</p>
    </footer>

    <script src="{% static 'js/script.js' %}"></script>
</body>
</html>


